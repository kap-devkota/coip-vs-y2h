---
title: "r-tests"
output: html_document
date: "2023-06-26"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("dplyr")
# install.packages("igraph")
library("igraph")
```

## Test 1: Are the Co-IP and Y2H networks structurally similar?

In this test, we try to find out how often does the triadic closure motiff occur in $G_c$ and $G_y$.

```{r test1-setup}
dgc <- read.csv("../data/networks/coip_hc_full.tsv", sep = "\t", header = FALSE)
dgy <- read.csv("../data/networks/y2h_hc_full.tsv", sep = "\t", header = FALSE)

# Ordering the columns of both Gc and Gy

dgc[which(dgc$V1 > dgc$V2), c("V1", "V2")] <- dgc[which(dgc$V1 > dgc$V2), c("V2", "V1")]
dgy[which(dgy$V1 > dgy$V2), c("V1", "V2")] <- dgy[which(dgy$V1 > dgy$V2), c("V2", "V1")]
```

Find $G$ from $G_c$ and $G_y$ by finding all the common nodes and edges between them.

```{r test1-findG}
dG <- merge(dgc, dgy, by.x = c("V1", "V2"), by.y= c("V1", "V2"), all.x = FALSE, all.y = FALSE)
dG[c("V1", "V2")]
```

Compute the actual iGraph $G$.

```{r test1-create-graph}
G <- graph_from_data_frame(dG[c("V1", "V2")], directed = FALSE) #, vertices = nodesG)
default_triangles <- length(triangles(G))
default_triangles
```

Now, we start the experiment by randomly adding edges in $G$ corresponding to $G_c$ and $G_y$

```{r test1-get-nodes}
Gnodes <- V(G)$name
Gcnodes <- union(dgc$V1, dgc$V2)
Gynodes <- union(dgy$V1, dgy$V2)
Gcmissing <- setdiff(Gcnodes, Gnodes)
Gymissing <- setdiff(Gynodes, Gnodes)

# DEBUG
c(length(Gnodes), length(Gcnodes), length(Gynodes), length(Gcmissing), length(Gymissing))


monte_carlo <- function(n_simulations, callback, ...) {
  simulations <- 1:n_simulations
  sapply(1:n_simulations, function(x){callback(...)})
}

# Given the test data-frame, common graph and sampled nodes, return the 
# length of triangles
sample_triangles <- function(dfGtest, Gcommon, missing_nodes, no_nodes, no_edges) {
  sampled_nodes <- sample(missing_nodes, no_nodes)
  nodestoadd <- union(sampled_nodes, V(Gcommon)$name)
  edgetoadd  <- dfGtest[which(dfGtest$V1 %in% nodestoadd & dfGtest$V2 %in% nodestoadd), c("V1", "V2")]
  edgetoadd <- edgetoadd[sample(nrow(edgetoadd), no_edges), ]
  gtoadd <- graph_from_data_frame(edgetoadd, directed = FALSE)
  g <- gtoadd %u% Gcommon
  return(length(triangles(g)))
}
```

Now we run tests for different edge and node combinations

```{r test1-run-gc}
gc_tcounts <- c()
E <- c(50, 100, 200, 300, 400, 500, 1000)
K <- c(10, 20, 30, 40, 50, 100)
n_simulations <- 50

results <- expand.grid(E, K)
colnames(results) <- c("Edges", "Nodes")
results$pvalues <- apply(results, MARGIN=1, function(a) {
  e <- a[1]
  k <- a[2]
  gc_counts <- monte_carlo(n_simulations, sample_triangles, 
                           dgc, G, Gcmissing, k, e)
  gy_counts <- monte_carlo(n_simulations, sample_triangles, 
                           dgy, G, Gymissing, k, e)
  res <- t.test(gc_counts, gy_counts)
  return(res$p.value)
})

write.table(results, "test1-results.tsv", sep = "\t", row.names = FALSE)
```

Seeing the results for different Edge and Node combinations, even after applying multiple testing correction (Bonnoferoni), the alternate hypothesis is still accepted for all edges and nodes combinations.

## Test 2: Are do the networks see new likely links differently, provided some link-prediction functions

Here, we test using two Link Prediction Algorithms:

1.  Common Weighted
2.  L3

```{r test2-setup}
dgc <- read.csv("../data/networks/coip_hc_full.tsv", sep = "\t", header = FALSE)
dgy <- read.csv("../data/networks/y2h_hc_full.tsv", sep = "\t", header = FALSE)

# Ordering the columns of both Gc and Gy

dgc[which(dgc$V1 > dgc$V2), c("V1", "V2")] <- dgc[which(dgc$V1 > dgc$V2), c("V2", "V1")]
dgy[which(dgy$V1 > dgy$V2), c("V1", "V2")] <- dgy[which(dgy$V1 > dgy$V2), c("V2", "V1")]
```

Get the shared version of $G_c$ and $G_y$, by selecting all the edges associated with common nodes between the two networks.
```{r test2-get-common}
gcnodes <- union(dgc[[1]], dgc[[2]])
gynodes <- union(dgy[[1]], dgy[[2]])

gcommon <- intersect(gcnodes, gynodes)
dgc_s <- dgc[which(dgc[[1]] %in% gcommon & dgc[[2]] %in% gcommon), ]
dgy_s <- dgy[which(dgy[[1]] %in% gcommon & dgy[[2]] %in% gcommon), ]

gcommon <- intersect(union(dgy_s$V1, dgy_s$V2), union(dgc_s$V1, dgc_s$V2))

list(dgc_s,dgy_s)
```

Now, define the link prediction functions `L3` and `CWN`
```{r test2-utilities}
library(Matrix)
library(MASS)
library(mnormt)


CWN <- function(mat) {
  deg <- apply(mat, 1, sum)
  Deg1_2 <- as(diag(1 / sqrt(deg)), "sparseMatrix")
  row.names(Deg1_2) <- rownames(mat)
  colnames(Deg1_2) <- colnames(mat)
  mask <- mat
  mask[mask > 0] <- 1
  cw <- mat %*% mask + mask %*% mat
  cw <- Deg1_2 %*% cw %*% Deg1_2
  return(cw)
}

L3 <- function(mat) {
  deg <- apply(mat, 1, sum)
  Deg1_2 <- as(diag(1 / sqrt(deg)), "sparseMatrix")
  mat1 <- Deg1_2 %*% mat %*% Deg1_2
  return(mat %*% mat1 %*% mat)
}

DSD <- function(mat, epsilon=1e-5, gamma = 1, normal=TRUE, dist = FALSE, sim = FALSE) {
  d <- apply(mat, 1, sum)
  
  d[d == 0] <- epsilon
  d_all <- sum(d)
  d_1 <- 1 / d
  P <- sweep(mat, 1, d_1, "*")
  
  e <- matrix(1, nrow=nrow(mat), ncol=1)
  W <- (1 / d_all) * (e %*% t(e * d))
  P1 <- gamma * (P - W)
  
  if (!is(P1, "matrix")) P1 <- as(P1, "matrix")
  
  X <- ginv(diag(nrow(mat)) - P1)
  
  if (normal) { X <- sweep(X, 2, sqrt(d_1), "*")}
  
  if (dist) {
    X <- dist(X, diag=TRUE, upper=TRUE)
    if (sim) {
      X <- exp(-X)
    }
  }
  
  X <- as(X, "matrix")
  
  rownames(X) <- rownames(mat)
  colnames(X) <- colnames(mat)
  return(X)
}
```

Now, we randomly generate node-pairs of size $K$, that are not present in either of the shared networks
```{r test2-generate-samples}
library("dplyr")
N <- 500
dgcy_s <- rbind(dgc_s[c("V1", "V2")], dgy_s[c("V1", "V2")])
dgcy_s <- dgcy_s[!duplicated(dgcy_s), ]

get_k_samples <- function(df, nodes, n_samples) {
  sampled_list <- as.data.frame(do.call(rbind, lapply(1:as.integer(n_samples * 1.5), function(i) {sample(nodes, 2, replace=FALSE)})))
  sampled_list[which(sampled_list$V1 > sampled_list$V2), c("V1", "V2")] <-  sampled_list[which(sampled_list$V1 > sampled_list$V2), c("V2", "V1")]
  return(anti_join(sampled_list, df, by=c("V1", "V2"))[1:n_samples, ])
}

colnames(results) <- c("No-samples", "Iteration")

Coip <- graph.data.frame(dgc_s, directed = FALSE)
Coip_mat <- get.adjacency(Coip, sparse = FALSE, attr='V3')
# Coip_mat <- as(Coip_mat, "sparseMatrix")

Y2h <- graph.data.frame(dgy_s, directed = FALSE)
Y2h_mat <- get.adjacency(Y2h, sparse = FALSE, attr='V3')
# Y2h_mat <- as(Y2h_mat, "sparseMatrix")


# CWMat_coip <- CWN(Coip_mat)
# CWMat_y2h <- CWN(Y2h_mat)

# L3Mat_coip <- L3(Coip_mat)
# L3Mat_y2h <- L3(Y2h_mat)

DSDMat_coip <- DSD(Coip_mat, dist = TRUE, sim = TRUE)
DSDMat_y2h <- DSD(Y2h_mat, dist = TRUE, sim = TRUE)
rownames(DSDMat_coip)
rownames(Coip_mat)


results <- expand.grid(c(1000, 2000, 3000, 5000, 10000), 1:5)
get_scores <- function(samples, mat, coln) {
  samples$coln <- apply(samples, 1, function(x) {return(mat[x[[0]], x[[1]]])})
}
scorr <- function(x, y) {
  xr <- rank(x)
  yr <- rank(y)
  return(cov(xr, yr) / (sd(xr) * sd(yr)))
}

results$p_values <- apply(results, 1, function(x){
  no_samples <- x[[1]]
  samples <- get_k_samples(dgcy_s, gcommon, no_samples)
  dsd_coip <- apply(samples, 1, function(x) {return(DSDMat_coip[x[[1]], x[[2]]])})
  dsd_y2h <- apply(samples, 1, function(x) {return(DSDMat_y2h[x[[1]], x[[2]]])})
  p_val <- wilcox.test(dsd_coip, dsd_y2h, alternative = "two.sided")$p.value
  return(p_val)
})

results$scor <- apply(results, 1, function(x) {
  no_samples <- x[[1]]
  samples <- get_k_samples(dgcy_s, gcommon, no_samples)
  dsd_coip <- apply(samples, 1, function(x) {return(DSDMat_coip[x[[1]], x[[2]]])})
  dsd_y2h <- apply(samples, 1, function(x) {return(DSDMat_y2h[x[[1]], x[[2]]])})
  spearman <- scorr(dsd_coip, dsd_y2h)
  return(spearman)
})

```

Visualizing Results
```{r test2-visualize}
library(ggplot2)
resplot <- results
colnames(resplot) <- c("Sampled Node-Pairs", "Iterations", "Spearman Correlation")
resplot$`Sampled Node-Pairs` <- as.factor(resplot$`Sampled Node-Pairs`)
p <- ggplot(resplot[c("Sampled Node-Pairs", "Spearman Correlation")], aes(x=`Sampled Node-Pairs`, y=`Spearman Correlation`)) +
  geom_boxplot(notch = FALSE) + scale_color_grey() + theme_classic()
p
```

## Test 3: 

The Test 3 shows how the local structure of CoIP and Y2H networks capture functionally similar proteins. For this, we use the 
MSigDB database

```{r test3-install-msigdb}
## Use these codes to install msigdb
# install.packages("BiocManager")
# BiocManager::install("msigdb")
# browseVignettes("msigdb")
```


```{r test3-imports}
library(msigdb)
library(ExperimentHub)
library(GSEABase)
library(org.Hs.eg.db)

INTERSECTING_THRESHOLD=10

msigdb.hs <- getMsigdb(org="hs", id="SYM", version='7.5')
msigdb.hs <- appendKEGG(msigdb.hs)

keggdb <- msigdb.hs[sapply(msigdb.hs, function(i){return(grepl("KEGG", setName(i), fixed = TRUE))})]

get_uniprot <- function(gset, commonprots){
  geneset <- select(org.Hs.eg.db, columns=c("SYMBOL", "UNIPROT"), keys=geneIds(gset), keytype="SYMBOL")$UNIPROT
  geneset <- sapply(geneset, function(i) {return(paste("uniprotkb:", i, sep=""))})
  return(intersect(geneset, commonprots))
}

# Loading graphs
dgc <- read.csv("../data/networks/coip_hc_full.tsv", sep = "\t", header = FALSE)
dgy <- read.csv("../data/networks/y2h_hc_full.tsv", sep = "\t", header = FALSE)

# Ordering the columns of both Gc and Gy

dgc[which(dgc$V1 > dgc$V2), c("V1", "V2")] <- dgc[which(dgc$V1 > dgc$V2), c("V2", "V1")]
dgy[which(dgy$V1 > dgy$V2), c("V1", "V2")] <- dgy[which(dgy$V1 > dgy$V2), c("V2", "V1")]

gcnodes <- union(dgc[[1]], dgc[[2]])
gynodes <- union(dgy[[1]], dgy[[2]])

gcommon <- intersect(gcnodes, gynodes)
dgc_s <- dgc[which(dgc[[1]] %in% gcommon & dgc[[2]] %in% gcommon), ]
dgy_s <- dgy[which(dgy[[1]] %in% gcommon & dgy[[2]] %in% gcommon), ]

gcommon <- intersect(union(dgy_s$V1, dgy_s$V2), union(dgc_s$V1, dgc_s$V2))

Gc <- graph.data.frame(dgc_s, directed=FALSE)
Gy <- graph.data.frame(dgy_s, directed=FALSE)

df <- data.frame(keggname = sapply(keggdb, function(i){setName(i)})) 
df$intersecting_nodes<-sapply(keggdb, function(i) {length(get_uniprot(i, gcommon))})
filtered_kegg <- keggdb[sapply(df$intersecting_nodes, function(i) {i >= INTERSECTING_THRESHOLD})]
length(filtered_kegg)

df <- df[which(df$intersecting_nodes >= INTERSECTING_THRESHOLD), ]
df$coip_edges <- sapply(filtered_kegg, function(i){
  commonnodes <- get_uniprot(i, gcommon)
  subG <- subgraph(Gc, commonnodes)
  return(gsize(subG))
})

df$y2h_edges <- sapply(filtered_kegg, function(i){
  commonnodes <- get_uniprot(i, gcommon)
  subG <- subgraph(Gy, commonnodes)
  return(gsize(subG))
})

df
```

Plot the `df` results
```{r test-3-plot-results}
library(ggplot2)
library(reshape2)
library(dplyr)

dfgg <- melt(df, id=c("keggname", "intersecting_nodes"))
colnames(dfgg) <- c("KEGG name", "Num. of Intersecting Nodes", "Net. Type", "Num. of Edges")

dflargestcoips <- df %>%
  arrange(desc(coip_edges)) %>%
  head(10)

dflargestcoipsgg <- melt(dflargestcoips, id=c("keggname", "intersecting_nodes"))
colnames(dflargestcoipsgg) <- c("KEGG name", "Num. of Intersecting Nodes", "Net. Type", "Num. of Edges")
dflargestcoipsgg$`KEGG name` <- sapply(dflargestcoipsgg$`KEGG name`, function(i){return(gsub("KEGG_", "", i))})

ggplot(dfgg, aes(x=`Num. of Intersecting Nodes`, y=`Num. of Edges`, color=`Net. Type`)) + 
  geom_point(shape=20) + theme_classic()



ggplot(dflargestcoipsgg, aes(x=`Num. of Intersecting Nodes`, y=`Num. of Edges`, color=`Net. Type`, label=`KEGG name`)) + 
  geom_point(shape=20) + theme_classic() + geom_text(hjust=0.7, vjust=0, size=2)

```


## Test 4: How does the CoIP and Y2H network represent signalling relationships?

This is done by finding out how different networks respond to 

```{r test4-get-data}
library(hgnc)
url <- latest_archive_url()
download_archive(url=latest_archive_url(),
                 path = getwd(),
                 filename = basename(url))

proteinsdf <- read.delim("hgnc_complete_set.txt", sep = "\t")
proteinsdf[which(sapply(proteinsdf$gene_group, function(i){grepl("protease", i, fixed = TRUE)})), ] #[which(proteinsdf$gene_goup)]
```
